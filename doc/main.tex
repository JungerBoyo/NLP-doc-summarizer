\documentclass{article}

\input{packages}
\input{commands}

\begin{document}

\begin{center}
\begin{tabular}{|c|c|}

\hline 
		Wydział Informatyki Politechniki Białostockiej & Data: 15.01.2024  \\
					   								   & Przetwarzanie języka naturalnego \\
\hline
		Projekt, Tworzenie podsumowań tekstu & Prowadzący: \\
		Piotr Zalewski, Kacper Świderek & dr inż. Tomasz Łukaszuk \\
\hline 

\end{tabular}
\end{center}

\section{Przedstawienie zadania projektowego}
Celem projektu było opracowanie prostego narzędzia do tworzenia 
podsumowań dokumentów tekstowych. Opracowane narzędzie daje możliwość 
wybrania metod/y użytej do tworzenia podsumowania oraz ew. porównania
jakości podsumowania na podstawie metryk BERTScore oraz ROUGE.
\section{Przedstawienie rozszerzenia zadania do oceny 5.0}
Rozszerzeniem zadania jest dodanie możliwości łączenia metod tworzących 
podsumowania, gdzie przy metodach ekstrakcyjnych metody łączone są
przy użyciu sumy ważonej wyników jakie każda metoda przypisuje do zdań
i następne wybranie N zdań z najwyższym wynikiem, z kolei w przypadku metod
abstrakcyjnych polega na zawężaniu obszerności podsumowania stopniowo
każdą kolejną metodą (kolejność podawana jest przez użytkownika). Przykładowo,
najpierw z tekstu A tworzone jest podsumowanie metodą X o wielkości 20\% tekstu 
pierwotnego, następnie wykorzystywana jest metoda Y, która na podstawie tekstu podsumowania
stworzonego za pomocą metody X, tworzy podsumowanie podsumowania obszerności 25\%
tego podsumowania. 
\section{Wprowadzenie teoretyczne (naukowe) do zagadnienia}
Przetwarzanie języka naturalnego to dziedzina skupiająca wiedzę z zakresu
językoznastwa oraz informatyki zajmująca się automatyzacją analizy, rozumienia,
tłumaczenia i generowania tekstu/języka naturalnego.
Podsumowanie tekstu polega na streszczeniu znaczenia tego tekstu, używając
innego tekstu, o mniejszym rozmiarze. Wtórnym do zagadnienia tworzenia podsumowań
jest zagadnienie analizy tekstu, czyli sposobu pozyskiwania i kodowania informacji o tekście
tj. występujące w nim słowa i semantyka. Wykorzystując metody kodowania informacji o tekście
można projektować algorytmy, które na podstawie tych reprezentacji generują podsumowania.
Wyróżnia się dwie grupy metod służących do tworzenia podsumowań, metody ekstrakcyjne,
polegające na wybraniu najbardziej znaczących zdań z tekstu oraz metody abstrakcyjne,
polegające na tworzeniu nowych zdań na podstawie tekstu. Z reguły, z racji na większą złożoność,
metody abstrakcyjne tworzą lepsze podsumowania, aczkolwiek ze względu na to, że tworzone
podsumowania to całkowicie nowy tekst, mogą zaistnieć w nim przekłamania tekstu pierwotnego. Przy metodach
ekstrakcyjnych wybierane są jedynie zdania kluczowe z tekstu, przez co są one w tym kontekście bezpieczniejsze.

\subsection{Metody ekstrakcyjne}
\subsubsection{Wybór na podstawie długości zdania}
Naiwną metodą typowania zdań do podsumowania jest tworzenie rankingu na podstawie
ilości słów w zdaniu (nie biorąc pod uwagę \textit{stop words} oraz różnych morfologii 
tych samych słów). Należy zauważyć, że przy tej metodzie ekstrakcyjnej podsumowanie
będzie zawsze możliwie nadłuższe.
\subsubsection{Wybór pierwszego i ostatniego zdania}
Kolejną naiwną metodą jest wybór pierwszego i ostatniego zdania bazując na założeniu, że
są one w większości przypadków podsumowywujące.
\subsubsection{TF-IDF}
TF-IDF to metoda reprezentacji tekstu oparta na bag-of-words. Pierwsza praca traktująca
o TF-IDF została opublikowana w 1972 \cite{sparckjones1972statistical}. Przy tej metodzie tekst 
reprezentowany jest jako macierz której każdy wiesz odpowiada danemu zdaniu z tekstu, a 
każda kolumna odpowiada danemu słowu. Wartości macierzy wyliczane są za pomocą wzoru (1).

\begin{center}
	\begin{equation}
		w_{ij} = tf_{ij} \cdot log(\frac{N}{df_i})
	\end{equation}
\end{center}

Gdzie w przypadku opisywanego rozwiązania $tf_{ij}$ oznacza ilość wystąpień słowa
$i$ w zdaniu $j$, N oznacza ilość zdań, a $df_i$ oznacza ilość zdań zawierających
słowo $i$. 
Przy tworzeniu podsumowania tworzony jest ranking w którym dla każdego zdania sumowane
są otrzymane wartości $w_{ij}$ zgodnie ze wzorem (2). Następnie wybierane jest $K$ 
zdań z najwyższym wynikiem.

\begin{center}
	\begin{equation}
		S_{j} = \sum_{i = 0}^{M} w_{ij}
	\end{equation}
\end{center}

\subsubsection{TextRank}
TextRank to algorytm bazujący na algorytmie PageRank opublikowany w 2004 roku \cite{mihalcea2004textrank}.
W algorytmie tworzony jest graf którego wierzchołki to, jak w opisywanym rozwiązaniu, zdania, słowa
kluczowe lub inne jednostki tekstu. W przypadku zdań krawędzie pomiędzy wierzchołkami reprezentują 
podobieństwo pomiędzy zdaniami. Podobieństwo może być określane chociażby za pomocą ilości wspólnych
słów/tokenów bądź przy użyciu podobieństwa wektorów wziętych np. z macierzy stworzonej za pomocą
algorytmu TF-IDF. Następnie przechodząc po grafie wylicza się wynik każdego wierzchołka według wzoru (3) \cite{mihalcea2004textrank}.

\begin{center}
	\begin{equation}
		WS(V_i) = 0.15 + 0.85 \cdot \sum_{V_j\in In(V_i)}^{} \frac{w_{ij}}{\sum_{V_k\in Out(V_j)}^{} w_{jk}} \cdot WS(V_j)
	\end{equation}
\end{center}

Jak w algorytmie PageRank we wzorze (3) dany wierzchołek otrzymuje tym wyższy wynik im więcej
wierzchołków nań wskazuje. Co więcej, wynik więrzchołków wskazujących na ten wierzchołek również
ma wpływ na wynik tego wierzchołka. We wzorze (3) dodatkowo dochodzą wspomniane wagi krawędzi, 
im wyższa waga tym większe podobieństwo pomiędzy tymi wierzchołkami tym większa kontrybucja 
do końcowego wyniku. Sortując wartości wierzchołków otrzymuje się listę zdań uporządkowaną malejąco
według stopnia reprezentatywności przez nie całego tekstu.

\subsection{Metody abstrakcyjne}

\subsubsection{T5}
T5 to model transformatorowy opracowany przez Google, który został zaprojektowany do obsługi szerokiego zakresu zadań NLP (przetwarzania języka naturalnego) w jednolity sposób.
Kluczową ideą T5 jest potraktowanie wszystkich zadań NLP jako zadań tekst-do-tekstu. Oznacza to, że zarówno zadania klasyfikacji, jak i generowania tekstu są formułowane jako zadania, w których model przyjmuje tekst wejściowy i generuje tekst wyjściowy.
Aby to osiągnąć, T5 wykorzystuje specjalne "prefiksowe" tokeny (np. "tłumaczenie na język niemiecki:", "streszczenie:") do odróżniania różnych zadań.
T5 bazuje na architekturze transformatora z enkoderem i dekoderem.
Wykorzystuje mechanizm uwagi (attention) i trenowany jest na ogromnym zbiorze danych tekstowych.
Model posiada różne warianty rozmiarowe, co pozwala na dostosowanie go do dostępnych zasobów obliczeniowych i konkretnych zadań \cite{DBLP:journals/corr/abs-1910-10683}.

\subsubsection{BART}
BART (Bidirectional and Auto-Regressive Transformers) to model zaproponowany przez Facebook AI w 2019 roku.
BART to model sekwencyjny do sekwencji (seq2seq), który został zaprojektowany do zadań wymagających zarówno kodowania kontekstu (rozumienia), jak i generowania tekstu.
Jest on często stosowany w zadaniach takich jak streszczanie tekstu, generowanie dialogów i tłumaczenie maszynowe. 
BART łączy cechy dwukierunkowego kodera (podobnego do BERT) z autoregresyjnym dekoderem (podobnym do modeli GPT).
BART składa się z kodera transformatorowego i dekodera transformatorowego. 
Koder jest dwukierunkowy i służy do kodowania kontekstu wejściowego. 
Dekoder jest autoregresyjny i generuje tekst wyjściowy token po tokenie, na podstawie zakodowanego kontekstu \cite{DBLP:journals/corr/abs-1910-13461}.

\subsubsection{Pegasus}
Pegasus (Pre-training with Extracted Gap-sentences for Abstractive Summarization) to model zaproponowany przez Google Research w 2020 roku. 
Pegasus to model pre-treningowy, który jest specjalnie zaprojektowany do streszczania abstrakcyjnego. 
Jego unikalna metoda pre-treningu opiera się na "maskowaniu" całych zdań z dokumentu i uczeniu się ich odtwarzania na podstawie pozostałego tekstu. 
Celem tego podejścia jest nauczenie modelu generowania streszczeń poprzez wyodrębnianie najważniejszych informacji i ich ponowne przedstawianie w skróconej formie.
Pegasus bazuje na architekturze transformatora sekwencja-do-sekwencji. Posiada enkoder i dekoder, podobnie jak BART, ale jego pre-trening jest dostosowany do streszczania \cite{DBLP:journals/corr/abs-1912-08777}.

\subsection{Ocena jakości podsumowań}
Najbardziej popularnymi metodami oceny jakości podsumowań jest \textit{ROUGE score} oraz
\textit{BARTScore}. Jakość metryki oceniana jest na podstawie jej korelacji z oceną
subiektywną (oceną dokonaną przez człowieka). Najlepszą metodą oceny podsumowania jest 
ocena subiektywna ale ze względu na to, że ręczne tworzenie podsumowań jest pracochłonne oraz że
ocena jest niestabilna (ciężko porównać dwa podsumowania i określić które z nich jest lepsze
i o ile), dąży się do wypracowania technik jak te wspomniane wyżej. 

\subsubsection{ROUGE score}
ROUGE score to metoda zaproponowana po raz pierwszy w 2004 roku \cite{lin2004rouge}. Jak już wspomniano,
przy ocenie jakości, podsumowanie dokonane ręcznie przez człowieka jest brane jako podsumowanie
wzorcowe. \textit{ROUGE score} opiera się na zliczaniu nakładających się jednostek tekstu takich jak
słowa, zdania czy n-gramy podsumowania wygenerowanego z podsumowaniem wzorcowym. Praca \cite{lin2004rouge}
wprowadza trzy miary \textit{ROUGE score}
\begin{itemize}
	\item \textit{ROUGE-N} - stosunek liczby n-gramów występujących zarówno w wygenerowanym podsumowaniu jak i w 
		podsumowaniach wzorcowych do sumy n-gramów z podsumowań wzorcowych.
	\item \textit{ROUGE-L} - na podstawie sumy najdłuższych wspólnych sekwencji wyliczanie 3 miar, 
		\textit{recall}, \textit{precision} i \textit{F}. Gdzie każda najdłuższa wspólna sekwencja jest
		wyliczana z danego zdania podsumowania referencyjnego i wszystkimi zdaniami z podsumowania wygenerowanego.
		Miary \textit{recall} i \textit{precision} to wspomniana suma podzielona przez kolejno długość podsumowania
		referencyjnego i wygenerowanego. Miarę \textit{F} określa się wzorem (4), gdzie $\beta$ to zmienna 
		kontrolująca czy \textit{recall} ($\beta > 1$) czy \textit{precision} ($\beta < 1$) jest bardziej 
		istotne w obliczonej wartości.
		\begin{center}
			\begin{equation}
				F = \frac{(1 + \beta^2) \cdot recall\cdot precision}{recall + \beta^2 \cdot precision}
			\end{equation}
		\end{center}
	\item \textit{ROUGE-W} - Ważona wersja \textit{ROUGE-L} biorąca pod uwagę
		ciągłość wspólnych sekwencji. Tym większa waga wspólnej sekwencji im większa jest jej ciągłość (\textit{ROUGE-L} ignoruje tą właściwość przez co dwie
		wykryte wspólne sekwencje, jedna ciągła a druga nie, mogą zostać ocenione
		tak samo mimo, że oczywistym jest, że ciągła sekwencja jest bardziej
		porządana).
	\item \textit{ROUGE-S} - Wyliczana jest liczba nakładających się \textit{skip-bigrams} 
		czyli par słów o dowolnej odległości w zdaniu ale występujących po sobie. Na podstawie tej 
		liczby podobnie jak w przypadku \textit{ROUGE-N} i \textit{ROUGE-L} wyliczane są
		\textit{recall}, \textit{precision} i \textit{F}.
\end{itemize}
Warto zaznaczyć, że miara ROUGE może negatywnie ocenić podsumowanie wygenerowane,
nawet gdy jest ono dobrej jakości, np. gdy zawiera ono podobną treść
co w podsumowaniu wzorcowym ale napisaną w inny sposób, używając innych słów itd.


\subsubsection{BARTscore}
BERTScore to metryka zaproponowana w 2019 roku \cite{zhang2020bertscoreevaluatingtextgeneration}. 
W metodzie tej używa się modelu BART do oceny podsumowania wygenerowanego na podstawie podsumowania
referencyjnego.

\section{Opis realizacji technicznej (wykorzystane biblioteki, dane treningowe)}
\subsection{Biblioteki}
Do wykonania zadania skorzystano z następujących bibliotek:
\begin{itemize}
	\item spacy - segmentacja tekstu, TextRank
	\item sklearn - TF-IDF
	\item transformers - abstrakcyjne podsumowania, BART, Pegasus, T5
	\item rouge\_score - wyliczanie rouge\_score podsumowania.
	\item bert\_score - wyliczanie bert\_score podsumowania.
\end{itemize}
\subsection{Dane treningowe}
Dane treningowe wytworzono korzystając z biblioteki gutenberg \cite{gutenberg}
zawierającej w pełni darmowe cyfrowe kopie książek tj. \textit{War and peace}, którym
posłużono się do tworzenia podsumowań. Podsumowania wzorcowe tworzono w większości przy pomocy
\textit{ChatGPT} ale również ręcznie.

\section{Instrukcja korzystania z programu}
Program do tworzenia podsumowań tekstu można uruchomić w trybie GUI lub w trybie konsolowym. Poniżej znajdują się instrukcje dla obu trybów.

\subsection{Tryb GUI}
Aby uruchomić program w trybie GUI, należy użyć flagi \texttt{-g} podczas uruchamiania skryptu \texttt{main.py}. Przykład:
\begin{verbatim}
python main.py -g
\end{verbatim}

Po uruchomieniu programu w trybie GUI, użytkownik zobaczy okno aplikacji, w którym można wprowadzić następujące informacje:
\begin{itemize}
    \item Ścieżka do pliku tekstowego: Wprowadź ścieżkę do pliku tekstowego, który ma zostać podsumowany.
    \item Typ podsumowania: Wybierz typ podsumowania (ekstrakcyjne lub abstrakcyjne).
    \item Metoda: Wybierz metodę podsumowania z listy dostępnych metod.
    \item Procent: Wprowadź liczbę określającą jaki procent całego teksu będzie stanowić podsumowanie(liczba całkowita np. 10). W przypadku podsumowania abstrakcyjnego, wartość ta może być inna dla każdej metody.
    \item Ścieżka do pliku referencyjnego (opcjonalne): Wprowadź ścieżkę do pliku zawierającego podsumowanie referencyjne.
    \item Katalog wyjściowy JSON (opcjonalne): Wprowadź ścieżkę do katalogu, w którym mają zostać zapisane wyniki w formacie JSON.
\end{itemize}

Po wprowadzeniu wszystkich wymaganych informacji, kliknij przycisk "Summarize", aby uruchomić proces tworzenia podsumowania. Wynik zostanie wyświetlony w oknie aplikacji.

\subsection{Tryb konsolowy}
Aby uruchomić program w trybie konsolowym, należy użyć następującej składni:
\begin{lstlisting}
python main.py -t <ścieżka_do_pliku_tekstowego> -m <ścieżka_do_modelu> -s <metoda> -p <procent> [-r <ścieżka_do_pliku_referencyjnego>] [-j <katalog_wyjściowy_JSON>]
do wyświetlenia pomocy można użyć: python main.py -h
\end{lstlisting}

Gdzie:
\begin{itemize}
    \item \texttt{-t <ścieżka\_do\_pliku\_tekstowego>}: Ścieżka do pliku tekstowego, który ma zostać podsumowany.
    \item \texttt{-m <ścieżka\_do\_modelu>}: Ścieżka do modelu używanego przez spaCy.
    \item \texttt{-s <metoda>}: Metody podsumowania.
    \item \texttt{-p <procent>}: Liczba określająca jaki procent całego teksu będzie stanowić podsumowanie.
    \item \texttt{[-r <ścieżka\_do\_pliku\_referencyjnego>]}: (Opcjonalne) Ścieżka do pliku zawierającego podsumowanie referencyjne.
    \item \texttt{[-j <katalog\_wyjściowy\_JSON>]}: (Opcjonalne) Ścieżka do katalogu, w którym mają zostać zapisane wyniki w formacie JSON.
\end{itemize}


Przykład:
\begin{lstlisting}
python main.py -t tekst.txt -m model -s EXT_IF_IDF -p 20 -r referencja.txt -o wyniki.json
\end{lstlisting}

Po uruchomieniu programu w trybie konsolowym, wyniki zostaną wyświetlone w konsoli oraz zapisane w katalogu wyjściowym (jeśli podano).

\section{Przykłady użycia programu (dane wejściowe i otrzymane wyniki)}
Aby zademonstrować działanie programu przanalizowano 3 zbiory tekstów z różnych książek z \cite{gutenberg}.
\begin{itemize}
	\item \textit{War and peace}, którego podsumowania zostały wygenerowane z użyciem \textit{chatGPT}
	\item \textit{Little prince}, którego podsumowania zostały wygenerowane z użyciem \textit{chatGPT}
	\item \textit{Moby Dick}, którego podsumowania zostały zebrane ze strony zajmującej się tworzeniem podsumowań
		książek \cite{sparknotes}
\end{itemize}
W każdym zbiorze znalazło się około 10 rozdziałów. Podsumowania wzorcowe jak i podsumowania generowane
przy użyciu różnych metod stanowią około 5\% tekstu oryginalnego.

\subsection{War and peace}
Na rys. 1 widać wykresy pudełkowe dla każdej metody dla dwóch metryk, \textit{ROUGE score, ROUGE-L, F1}
oraz \textit{BERTScore, F1}. Wybrano metryki F1 z każdej metody oceny ponieważ zawierają one zarówno
\textit{precision} jak i \textit{recall}.

\img[0.4]{war_and_peace.png}{Wykresy pudełkowe, z wyników na danych rozdziałach książki
\textit{War and peace}. Po lewej \textit{ROUGE score, ROUGE-L}, po prawej \textit{BERTScore, F1}}

\subsection{Little prince}
Na rys. 2 widać wykresy pudełkowe dla każdej metody dla dwóch metryk, \textit{ROUGE score, ROUGE-L} oraz 
\textit{BERTScore, F1}.
\img[0.4]{little_prince.png}{Wykresy pudełkowe, z wyników na danych rozdziałach książki
\textit{Little prince}. Po lewej \textit{ROUGE score, ROUGE-L}, po prawej \textit{BERTScore, F1}}

\subsection{Moby dick}
Na rys. 3 widać wykresy pudełkowe dla każdej metody dla dwóch metryk, \textit{ROUGE score, ROUGE-L} oraz 
\textit{BERTScore, F1}.
\img[0.4]{mobydick.png}{Wykresy pudełkowe, z wyników na danych rozdziałach książki
\textit{Moby dick}. Po lewej \textit{ROUGE score, ROUGE-L}, po prawej \textit{BERTScore, F1}}

\subsection{Moby dick - ewaluacja łączenia metod}
Na rys. 4 widać wykresy pudełkowe dla każdej metody łączenie z wybranymi kombinacjami dla dwóch metryk, 
\textit{ROUGE score, ROUGE-L} oraz \textit{BERTScore, F1}. 
\img[0.4]{final.png}{Wykresy pudełkowe, z wyników na danych rozdziałach książki
\textit{Moby dick}. Po lewej \textit{ROUGE score, ROUGE-L}, po prawej \textit{BERTScore, F1}}

\subsection{Wnioski}
Na podstawie wyników z rys. 2, można stwierdzić, że podsumowania ekstrakcyjne
mogą przynosić zadowalające rezultaty gdy teksty/dokumenty są krótkie (rozdziały w 
książce \textit{Little Prince}, są najkrótsze spośród wybranych źródeł). \\

Zaskakująco dobrze wypadła metoda \textit{FIRST LAST} czyli polegająca na wybraniu
pierwszego i ostatniego zdania z tekstu. Możliwym wyjaśnieniem tych wyników
jest to, że przy tej metodzie nie da się kontrolować objętości podsumowania. Oznacza
to, że zazwyczaj ich długość jest mniejsza niż 5\% oryginalnego tekstu. Przy obliczaniu
metryki F1 brana jest wartość \textit{precision}, która w mianowniku zawiera 
długość wygenerowanego podsumowania, może to doprowadzić do sztucznego polepszenia wyniku.

Na rys. 1, 3, wyłączając metodę ekstrakcyjną \textit{FIRST LAST}, widać, że podsumowania 
abstrakcyjne są ogólnie lepsze od metod ekstrakcyjnych.
Metoda \textit{Pegasus} otrzymała
najwyższe wyniki, jedynym wyjątkiem jest BERTScore metody BERT przy podsumowaniach 
\textit{War and peace}.\\

Poniżej przedstawiono podsumowania wygenerowane metodą \textit{Pegasus}, \textit{FRIST LAST} oraz
podsumowanie referencyjne. Podsumowanie \textit{FIRST LAST} uchwytuje słynne pierwsze zdanie,
które zostało nawet zaznaczone w podsumowaniu referencyjnym (zdecydowanie szczególny przypadek).
Podsumowanie \textit{Pegasus} uchwyciło na swój sposób czemu \textit{whaling voyage was welcome}
czego zabrakło w podsumowaniu \textit{FIRST LAST}. Nie porządaną cechą podsumowań abstrakcyjnych 
są powtarzające się słowa/frazy/zdania pojawiające się regularnie, np. poniżej fragment
\textit{If you ever want to see the sea, go to the Isles of Scilly, to the Isles of Why do we go
to sea so often, so often, so often, so often, so often?}
\paragraph{FIRST LAST}
\begin{center}
\begin{lstlisting}
Call me Ishmael.  By reason of these things, then, the whaling voyage was welcome; the great flood-gates of the wonder-world swung open, and in the wild conceits that swayed me to my purpose, two and two there floated into my inmost soul, endless processions of the whale, and, mid most of them all, one grand hooded phantom, like a snow hill in the air.
\end{lstlisting}
\end{center}
\paragraph{Pegasus}
\begin{center}
\begin{lstlisting}
In our series of letters from British journalists, film-maker and columnist John Humphrys reflects on If you ever want to see the sea, go to the Isles of Scilly, to the Isles of Why do we go to sea so often, so often, so often, so often, so often? I always go to sea as a sailor, because they make a point of paying me for my trouble, My dear readers, I would like to tell you a little bit about the Fates, and how they The whaling voyage was welcome; the great flood-gates of the wonder-world swung open
\end{lstlisting}
\end{center}
\paragraph{Reference}
\begin{center}
\begin{lstlisting}
The narrative of Moby-Dick begins with the famous brief sentence, ``Call me Ishmael.`` Ishmael, a sailor, describes a typical scene in New York City, with large groups of men gathering on their days off to contemplate the ocean and dream of a life at sea. He explains that he himself went to sea because, like these men, he was feeling a ``damp, drizzly November in [his] soul`` and craved adventure. Shunning anything too ``respectable`` (or expensive), he always ships as a common sailor rather than as a passenger.
\end{lstlisting}
\end{center}

Na rys. 4 przedstawiono również wyniki otrzymane 
przy zastosowaniu metod łączonych. Na podstawie wykresów
można określić, że jedyne połączenie które przyniosło
dobre rezultaty to zastosowanie najpierw metody BERT,
a następnie metody Pegasus. Należy jednak zaznaczyć, że
wspomniane zapętlanie się w powtarzaniu fraz nasila się 
przy zaproponowanym łączeniu metod abstrakcyjnych.
% Czym się różni się lemantyzacja od stemmingu (lub co to stemming/lemantyzacja
% Co to jest odległość edycyjna (lavenshtain)
% Wady i zalety reprezentacji tekstu bag-of-words

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
